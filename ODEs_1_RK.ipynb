{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484883d2-0577-4191-a973-a2294e708afc",
   "metadata": {},
   "source": [
    "# 7. Ordinary differential equations II\n",
    "\n",
    "\n",
    "## 7.1 Recap\n",
    "\n",
    "Numerical libraries have plenty of methods for solving initial value problems, which they usually ask for in the standard form\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{u}'(t) &= f(\\mathbf{u}(t)), \\quad t \\in [a, b], \\\\\n",
    "\\mathbf{u}(a) &= \\mathbf{u}_0,\n",
    "\\end{align}\n",
    "\n",
    "i.e. as a system of first-order ODEs. They will **require** as inputs\n",
    "* the right-hand-side $f(t, u)$,\n",
    "* the solution interval $[a, b]$,\n",
    "* and the initial state $u_0$.\n",
    "Optionally, you can usually set\n",
    "* the relative and absolute tolerances of the method,\n",
    "* the order of the method (this determines the order of accuracy),\n",
    "* whether to interpolate the solution onto a dense grid (dense output),\n",
    "* and sometimes some special stopping conditions.\n",
    "\n",
    "IVP solution methods are generally \"marching\" or \"time-stepping\" methods with 3 components,\n",
    "* A method to approximate / forecast the unknown $u$ with,\n",
    "* A method to estimate the error of this approximation,\n",
    "* And a method that uses this error estimate to control parameters of the solver so as to keep the error below a user-specified _tolerance_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6558f31-d68c-4d4d-b65e-dfe76d452e38",
   "metadata": {},
   "source": [
    "## 7.2 Forecasting and (unit) local truncation error\n",
    "\n",
    "For describing the forecasting function of each method, we'll use the following notation. The exact solution will be denoted $u$, the numerical approximation of which is $\\tilde{u}$. The timesteps taken by the solver are $\\{t_i\\}_{i=0}^{n}$, with the length of steps (not necessarily equal) being $\\{h_i\\}_{i=0}^{n}$, where $h_i = t_{i+1} - t_i$. The forecasting function is defined as\n",
    "\n",
    "$$ \\tilde{u}_{i} = \\tilde{u}_{i-1} + h_{i-1}\\Phi(t_i, t_{i-1}, \\ldots; \\tilde{u}_i, \\tilde{u}_{i-1},\\ldots;f). $$\n",
    "\n",
    "The simplest forecasting functions belong to the forward Euler method,\n",
    "$$ \\tilde{u}_{i} = \\tilde{u}_{i-1} + h_{i-1}f(\\tilde{u}_{i-1},t_{i-1}), $$\n",
    "and the backward Euler method,\n",
    "$$ \\tilde{u}_{i} = \\tilde{u}_{i-1} + h_{i-1}f(\\tilde{u}_{i},t_{i}). $$\n",
    "Notice how the second term in each of these is just the stepsize times $\\tilde{u}$ evaluated at different stages of the step (the start and the end, respectively).\n",
    "\n",
    "The IVP solvers usually control the error they accumulate during a single timestep, i.e. their _local truncation error_ (LTE). This is defined as\n",
    "\n",
    "$$ \\Delta_i = u(t_{i+1}) - \\big( u(t_i) + h_i\\Phi(t_i, t_{i-1}, \\ldots; u_i, u_{i-1},\\ldots;f) \\big), $$\n",
    "\n",
    "i.e. it is the difference between the exact and the numerically predicted solution if one started the timestep from the exact solution. Its unit-length sibling is the ULTE,\n",
    "\n",
    "$$ \\delta_i = \\frac{\\Delta_i}{h_i}. $$\n",
    "\n",
    "One can anticipate the length of timesteps taken by the forward Euler method by considering how quickly its LTE converges as a function of $h$. Expressing $u(t_{i+1})$ as a Taylor series around $t_{i-1}$, we see that the LTE is\n",
    "\n",
    "$$ \\Delta_i = \\frac{1}{2}h^2\\tilde{u}_i + \\mathcal{O}(h^3), $$\n",
    "\n",
    "so the ULTE is $\\mathcal{O}(h)$. So the method should converge, albeit slowly, for small enough $h$ (though too small an $h$ will usually lead to the amplification of roundoff error, i.e. conditioning issues).\n",
    "\n",
    "Generally, there is no reliable way for an IVP solver to control the _global_ error in the solution, as this depends too much on the problem itself. But under special conditions, the global error can be shown to be roughly a factor of $h$ slower in converging to zero than the local error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5865d6af-4635-4f77-9671-da6c2e075e05",
   "metadata": {},
   "source": [
    "```{admonition} Theorem\n",
    ":class: hint\n",
    "\n",
    "Take an IVP solving method with constant stepsize $h$. If its LTE satisfies\n",
    "$$ \\delta_i(h) = ch^p, $$\n",
    "and\n",
    "$$ \\frac{\\partial \\Phi}{\\partial u} \\geq L \\quad \\forall t \\in [a, b], \\; \\forall u, \\; \\text{and }\\forall h >0,  $$\n",
    "then the global error is bounded as\n",
    "$$ |u(t_i) - \\tilde{u}_i| \\geq \\frac{ch^p}{L}\\left[ e^{L(t_i - a)} - 1\\right] = \\mathcal{O}(h^p) \\quad \\text{as } h \\to 0. $$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ed1a7f-8875-476c-a6a8-507fca1ebcd2",
   "metadata": {},
   "source": [
    "** Proof: ** Define the gloal error sequence $\\{\\varepsilon\\}_i$ as\n",
    "$$ \\varepsilon_i = u(t_i) - \\tilde{u}_i. $$\n",
    "Then \n",
    "$$ \\varepsilon_{i+1} - \\varepsilon_i = u(t_{i+1}) - u(t_i) - (\\tilde{u}_{i+1} - \\tilde{u}_i) = u(t_{i+1}) - u(t_i) - \\Phi(\\tilde{u}_i,t_i). $$\n",
    "Using the definition of the LTE, we can rewrite this as \n",
    "$$ \\varepsilon_{i+1} = \\varepsilon_i + \\Delta_i + h[\\Phi(u(t_i),t_i) - \\Phi(\\tilde{u}_i,t_i) ]. $$\n",
    "The fundamental theorem of calculus lets us write the absolute value of the second term on the RHS as\n",
    "$$ |\\Phi(u(t_i),t_i) - \\Phi(\\tilde{u}_i,t_i)| = \\left| \\int_{\\tilde{u}_i}^{u(t_i)} \\frac{\\partial \\Phi}{\\partial u}\\mathrm{d}u \\right|, $$\n",
    "which can then be bounded as \n",
    "$$ |\\Phi(u(t_i),t_i) - \\Phi(\\tilde{u}_i,t_i)| \\leq  \\int_{\\tilde{u}_i}^{u(t_i)} \\left|\\frac{\\partial \\Phi}{\\partial u}\\right|\\mathrm{d}u \\leq L|u(t_i) - \\tilde{u}_i| = L|\\varepsilon_i|, $$\n",
    "where we used the assumption of the theorem. \n",
    "Using the other assumption of the theorem (about the form of the LTE) to bound the second term in $|\\varepsilon_{i+1}|$, we get\n",
    "$$ |\\varepsilon_{i+1}| \\leq |\\varepsilon_i| + ch^{p+1} + hL|\\varepsilon_i|. $$\n",
    "Collecting terms, we arrive at the recursion relation\n",
    "$$ |\\varepsilon_{i+1}| \\leq ch^{p+1} + (1 + hL)|\\varepsilon_i|, $$\n",
    "in which we can continue replacing the $\\varepsilon_i$ terms with $\\varepsilon_{i-1}, \\varepsilon_{i-2}, \\ldots$, and noting that $\\varepsilon_0 = $, we get\n",
    "$$ |\\varepsilon_{i+1} \\leq ch^{p+1} [1 + (1 + hL) + (1 + hL)^2 + \\ldots + (1 + hL)^i]. $$\n",
    "Summing the geometric progression in square brackets and replacing $i+1 \\to i$, we obtain\n",
    "$$ |\\varepsilon_i| \\leq \\frac{ch^p}{L}[(1 + hL)^i - 1] \\leq \\frac{ch^p}{L}[e^{ihL} - 1], $$\n",
    "which is the bound we wanted to show, with $ih = (ti - a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b65f2-1b68-4748-9ee1-2cd5747428eb",
   "metadata": {},
   "source": [
    "There are several things we can deduce from the above expression:\n",
    "* convergence in the global error will be (at least) one order slower than in the local error,\n",
    "* as $i \\to \\infty$, or, equivalently, $t_i \\to \\infty$ (i.e. for long solution intervals), global error can accumulate exponentially,\n",
    "* however, the proof relies on uniform stepsizes, which generally won't be true (we mostly use adaptive solvers).\n",
    "\n",
    "Even though this proof works with uniform stepsizes, the conclusions generally hold. Let's now look at some examples of how one would get $\\Delta \\propto h^{p+1}$-type local truncation error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f465fb-01ab-4e12-878b-6c9c762c4ef4",
   "metadata": {},
   "source": [
    "## 7.3 Runge--Kutta methods\n",
    "\n",
    "Most large numerical libraries will have a _Runge--Kutta_ method as a default ODE (initial value problem) solver. \n",
    "These methods all work on the basis of matching the first given number of term's in $u$'s Taylor expansion around the current timestep, for example,\n",
    "$u(t_{i+1}) \\approx u(t_i) + hu'(t_i) + \\frac{1}{2}h^2 u''(t_i) + \\mathcal{O}(h^3).$\n",
    "Out of the RHS terms, $u'$ is given thorugh the ODE itself: $u'(t_i) = f(t_i, u(t_i))$. The next one contains $u''$, which is a little trickier, since we have to keep in mind that $f$ is in general a function of multiple variables ($t$ and $u$), and so\n",
    "$$ u'' = \\frac{\\mathrm{d} f}{\\mathrm{d} t} = \\frac{\\partial f}{\\partial t} + \\frac{\\partial f}{\\partial u}\\frac{\\mathrm{d} u}{\\mathrm{d} t} := f_t + f_u f. $$\n",
    "Plugging these back into the Taylor expansion, we get\n",
    "$$ u(t_{i+1}) \\approx u(t_i) + h \\left[ f(t_i, u(t_i)) + \\frac{h}{2} f_t(t_i, u(t_i)) + \\frac{h}{2}f(t_i, u(t_i))f_u(t_i, u(t_i)) \\right] + \\mathcal{O}(h^3). $$\n",
    "We see that as we try to match more and more terms in the Taylor expansion, more expressions for the higher derivatives of $f$ are required. While these can in theory be provided symbolically (or via automatic differentiation) by the user, it would be error-prone and cumbersome.\n",
    "\n",
    "The trick is to combine **intermediate** evaluations of $f$, which if Taylor-expanded around $t_i$, $u(t_i)$, yield these terms as follows,\n",
    "$$ f(t_i + \\alpha, u(t_i) + \\beta) \\approx f(t_i, u(t_i)) + \\alpha f_t(t_i, u(t_i)) + \\beta f_u(t_i, u(t_i)) + \\mathcal{O}(\\alpha^2 + |\\alpha \\beta| + \\beta^2). $$\n",
    "\n",
    "Now if we pick $\\alpha = \\frac{h}{2}$ and $\\beta = \\frac{h}{2}f(t_i, u(t_i))$, we see that\n",
    "$$ u_(t_{i+1}) = u(t_i) + h[f(t_i + \\alpha, u(t_i) + \\beta)] + \\mathcal{O}(h^3). $$\n",
    "(Check this yourself!)\n",
    "\n",
    "More generally, we can match even more terms in $u$'s Taylor expansion by making several intermediate $f$-evaluations. These intermediate evaluations constitute **stages** of a Runge--Kutta method. For $s$ stages, the forecasting step can be summarized as (note that I'll write $u_i := u(t_i)$)\n",
    "\\begin{align}\n",
    "&k_1 = hf(t_i, u_i),\\\\\n",
    "&k_2 = hf(t_i + c_1h, u_i + a_{11}k_1 + a_{12}k_2 + \\ldots + a_{1s}k_s),\\\\\n",
    "&\\vdots \\\\\n",
    "&k_i = hf\\left(t_i + c_ih, u_i + \\sum_{j = 1}^s a_{ij}k_j\\right), \\quad \\text{until } i=s,\\\\\n",
    "&\\text{then}\\\\\n",
    "&u_{i+1} = u_i + \\sum_{i=1}^s b_i k_i,\n",
    "\\end{align}\n",
    "where the $a_{ij}$, $b_i$, and $c_i$ are all parameters of the Runge--Kutta method. They are conventionally summarized in a Butcher tableau,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad88fc-6438-4590-bea6-a3f8f83d8a0e",
   "metadata": {},
   "source": [
    "```{list-table}\n",
    ":header-rows: 4\n",
    "\n",
    "* - $c_1$  \n",
    "  - $a_{11}$ \n",
    "  - $a_{12}$ \n",
    "  - $\\ldots$ \n",
    "  - $a_{1s}$\n",
    "* - $c_2$  \n",
    "  - $a_{21}$ \n",
    "  - $\\ddots$\n",
    "  - $ $\n",
    "  - $ $\n",
    "* - $\\vdots$  \n",
    "  - $\\vdots$\n",
    "  - $ $\n",
    "  - $ $\n",
    "  - $ $\n",
    "* - $c_s$  \n",
    "  - $a_{s1}$   \n",
    "  - $ $\n",
    "  - $ $\n",
    "  - $a_{ss}$ \n",
    "* - $ $\n",
    "  - $b_1$ \n",
    "  - $\\ldots$\n",
    "  - $ $\n",
    "  - $b_s$   \n",
    "* - $ $ \n",
    "  - $b_1^{\\ast}$\n",
    "  - $\\ldots$ \n",
    "  - $ $\n",
    "  - $b_s^{\\ast}$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f245471-91ea-4add-bc2c-3cb7b7398242",
   "metadata": {},
   "source": [
    "The method is called **explicit** if $a_{ij} = 0$ for $j \\geq i$, in which case the $k_i$ may be evaluated sequentially (without the need to solve a linear system). Since most operations involved in this calculation are fundamental, the dominant cost of a single step of an explicit Runge--Kutta method will be determined by **the number of $f$-evaluations** per step, which is $s$. \n",
    "\n",
    "Otherwise, the method is **implicit**. In this case, we see that to evaluate the $k_i$, a linear system of size (at most) $s$ by $s$ needs to be solved, the cost of which may be comparable to the $f$-evaluations (depending on how complex $f$ is). \n",
    "\n",
    "Butcher tableaus may have an extra bottom row, containing $b_i^{\\ast}$. These are an alternative set of coefficients for making the linear combination\n",
    "$u_{i+1}^{\\ast} = \\sum_{i = 1}^s b_i^{\\ast} k_i,$\n",
    "where $u_{i+1}^{\\ast}$ will typically be another estimate for $u_{i+1}$, accurate to some $\\mathcal{O}(h^{p'+1})$, where $p'$ is within a couple integers of $p$. Its use is that $|u_{i+1} - u_{i+1}^{\\ast}|$ provides a convenient error estimate for the LTE and may be used to update the stepsize $h$ (see later).\n",
    "\n",
    "The $a_{ij}$, $b_i$, and $c_i$ completely determine a Runge--Kutta method and are set in a manner similar to how we chose $\\alpha$ and $\\beta$. They have to obey a set of generally _nonlinear_, _algebraic_ conditions called the **order conditions**. These conditions will typically leave a couple of degrees of freedom, i.e.\\ there will be more parameters than constraints, and these remaining parameters are set by optimizing error constants. \n",
    "\n",
    "Very high-order Runge--Kutta methods are not typically used because one requires an increasingly large number of stages $s$ to satisfy the order conditions to $p$-th order (resulting in $\\mathcal{O}(h^{p+1})$ error, and a \"$p$-th order\" method). The following table summarizes how many stages are necessary for a $p$-th order method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab0feb-ed29-41e5-909d-595d35bb0bc4",
   "metadata": {},
   "source": [
    "| Stages | Order |\n",
    "|:----:|:------:|\n",
    "| $s \\leq 4$ | $p = s$| \n",
    "| $ 5 \\leq s \\leq 7 $ | $p = s-1$ | \n",
    "| $8 \\leq s \\leq 9 $ | $p = s-2$ | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068be2c-4d8b-44e1-bfd6-7d8c5ff311f7",
   "metadata": {},
   "source": [
    "## 7.4 Adaptive stepsize\n",
    "\n",
    "Let's revisit how the $i$-th stepsize, $h_i$, is selected in something like a $p$-th order Runge--Kutta method, whose LTE varies as\n",
    "$$ \\Delta_i = ch^{p+1}. $$\n",
    "Assuming that $c$ is constant during the solution of the ODE, i.e. doesn't vary with $t$ (this is **not** true in general as it depends on $f$), let's assume the previous step, of size $h_{i-1}$, had local error $\\Delta_{i-1}$, and that we wish to keep the LTE below a tolerance $\\varepsilon$, supplied by the user. Then the largest possible next step we can take has an LTE of exactly $\\varepsilon$. This means\n",
    "$$ \\Delta_i = \\varepsilon = ch_i^{p+1}, $$\n",
    "where we can get $c$ by expressing it from the LTE of the previous step,\n",
    "$$ \\Delta_{i-1} = ch_{i-1}^{p+1} \\quad \\rightarrow \\quad c = \\frac{\\Delta_{i-1}}{h_{i-1}^{p+1}}, $$\n",
    "and therefore get\n",
    "$$ h_i = \\left( \\frac{\\Delta_{i-1}}{\\varepsilon} \\right)^{\\frac{1}{p+1}}h_{i-1}. $$\n",
    "Thus if $\\Delta_{i-1} < \\varepsilon$ (the error was within tolerance), the step is **accepted** and the next stepsize is **increased**, whereas if $\\Delta_{i-1} > \\varepsilon$, the step should **not be accepted** (LTE was above tolerance), and instead should be **retried** with the now **reduced** $h_i$. \n",
    "\n",
    "In practice, there are several safety factors to ensure that steps don't get rejected. You can read about them in [Numerical Recipes, Ch 17.2 \"Adaptive Stepsize Control for Runge--Kutta\"](https://numerical.recipes/book.html). \n",
    "\n",
    "Stepsize control theory is a rich field that deals with the construction of various filters for getting rid of oscillatory behaviour in the stepsizes, which you can read about [here](https://link.springer.com/article/10.1023/A:1021160023092)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76124e-dabe-4fd8-8e93-b445a671dea3",
   "metadata": {},
   "source": [
    "## 7.5 Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34851f8",
   "metadata": {},
   "source": [
    "```{admonition} Exercise\n",
    ":class: danger\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3412c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
